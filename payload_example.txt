    # payload: (dictionary)
    # 1. encoder_cat -> long (batch_size x n_encoder_time_steps x n_features) -> long tensor of encoded categoricals for encoder
    # 2. encoder_cont -> float tensor of scaled continuous variables for encoder
    # 3. encoder_lengths -> long tensor with lengths of the encoder time series. No entry will be greater than n_encoder_time_steps
    # 4. encoder_target -> if list, each entry for a different target. float tensor with unscaled continous target or encoded categorical target, list of tensors for multiple targets
    # 5. decoder_cat -> long tensor of encoded categoricals for decoder
    # 6. decoder_cont -> float tensor of scaled continuous variables for decoder
    # 7. decoder_lengths -> long tensor with lengths of the decoder time series. No entry will be greater than n_decoder_time_steps
    # 8. decoder_target -> if list, with each entry for a different target. float tensor with unscaled continous target or encoded categorical target for decoder - this corresponds to first entry of y, list of tensors for multiple targets
    # 9. target_scale -> if list, with each entry for a different target. parameters used to normalize the target. Typically these are mean and standard deviation. Is list of tensors for multiple targets.


    '''
    EXAMPLE:

    d['encoder_cat']: tensor([], size=(1, 300, 0), dtype=torch.int64)

    d['encoder_cont']: tensor([[[-1.7313,  1.4117, -1.1577,  ..., -0.1097, -0.9606, -0.9533],
         [-1.7299, -1.4476, -1.1545,  ..., -0.1097, -0.9594, -0.9521],
         [-1.7285, -0.7327, -1.1646,  ..., -0.1097, -0.9619, -0.9543],
         ...,
         [-1.3086, -0.0179, -1.0813,  ...,  0.6847, -0.9084, -0.8963],
         [-1.3072,  0.6969, -1.0827,  ...,  0.2701, -0.9082, -0.8967],
         [-1.3058,  1.4117, -1.0520,  ...,  0.5260, -0.9040, -0.8949]]]) shape: torch.Size(1, 300, 13)
    
    d['encoder_lengths']: tensor([300])

    d['encoder_target']: tensor([[41.8300, 41.9400, 41.5900, 41.5900, 41.8000, 42.5000, 42.6200, 41.9100,
         41.8100, 42.0500, 42.0400, 42.6300, 42.3700, 42.6100, 42.3200, 42.5000,
         42.0600, 42.6100, 41.7500, 41.1900, 40.9200, 41.1100, 41.1000, 40.5400,
         40.3100, 39.9500, 40.1200, 40.5200, 40.5300, 39.8400, 39.5200, 39.7900,
         39.6000, 39.9000, 39.4900, 39.8100, 39.6000, 40.1900, 40.0500, 40.0200,
         38.7400, 38.5800, 39.2600, 39.5800, 39.3600, 39.7900, 38.7500, 38.9200,
         39.4000, 39.9600, 40.4900, 40.1300, 40.3400, 39.9500, 40.0600, 39.5700,
         40.3300, 40.2500, 39.3100, 39.6100, 40.4500, 40.4000, 40.9700, 40.9500,
         41.0000, 40.6200, 40.7200, 40.1200, 39.9900, 40.4700, 39.4200, 38.8900,
         38.5600, 38.6500, 38.8200, 39.0200, 38.4000, 37.5100, 36.2300, 34.6800,
         34.3600, 35.6200, 36.6400, 36.5100, 36.3100, 34.7500, 35.5300, 35.7500,
         35.0600, 36.2100, 35.5400, 35.8600, 35.9600, 35.5500, 36.1500, 36.5200,
         36.4000, 35.7400, 35.6900, 35.0500, 34.9500, 34.5500, 34.4500, 33.3700,
         33.7400, 34.3300, 33.7400, 34.6700, 35.3400, 34.9000, 35.5400, 36.0100,
         36.2300, 35.9900, 35.6300, 35.0500, 35.5800, 35.7800, 36.2300, 36.3200,
         35.9000, 36.0900, 37.1100, 36.8300, 37.0500, 37.5200, 37.7000, 37.7600,
         38.5900, 38.2700, 38.3400, 38.3000, 38.1400, 37.9200, 37.9800, 37.6600,
         37.4900, 36.7700, 37.3300, 38.3800, 39.3400, 38.5000, 39.2800, 40.0300,
         40.6700, 41.3400, 41.9700, 41.8200, 41.0600, 40.4200, 40.1500, 41.1000,
         40.7000, 41.1700, 40.9800, 41.2500, 40.1900, 40.2500, 40.8100, 41.4300,
         40.7800, 40.1000, 40.8300, 41.2200, 41.7700, 42.1400, 41.7800, 42.3600,
         42.1700, 41.8100, 40.6900, 40.5500, 40.7300, 39.0000, 38.5900, 37.9400,
         38.1900, 36.8600, 37.6100, 37.1100, 37.2000, 37.2600, 37.2300, 37.9800,
         37.6100, 37.5100, 37.0500, 36.1100, 37.6500, 37.6900, 37.0700, 37.1900,
         37.4200, 36.0400, 34.8000, 35.3700, 35.8500, 35.3300, 36.2200, 37.0800,
         37.8700, 37.1900, 37.4400, 38.0300, 37.1700, 37.4800, 37.6300, 37.5900,
         37.3500, 38.5900, 39.0400, 39.2000, 39.3400, 39.4300, 38.0900, 38.1300,
         37.9800, 39.2300, 39.0200, 38.2900, 38.9800, 39.2900, 40.1900, 39.6500,
         39.8800, 39.4700, 39.4300, 39.5200, 40.1600, 40.1200, 39.8500, 40.3100,
         40.0100, 39.2600, 40.2400, 39.4700, 39.5400, 39.4100, 39.7500, 40.6900,
         40.7800, 40.9100, 41.3100, 41.4000, 41.5200, 41.9300, 41.8000, 41.6300,
         41.3200, 41.5800, 41.1800, 40.9200, 41.7100, 41.2400, 40.5200, 40.9600,
         41.4300, 41.3300, 42.0000, 42.4600, 42.7300, 42.6200, 42.9400, 44.4100,
         44.3200, 44.5600, 44.8900, 44.5700, 45.0300, 45.4200, 45.5100, 45.8200,
         45.8900, 45.9500, 45.9000, 45.7200, 45.8600, 45.7500, 46.1500, 46.0900,
         45.3600, 44.9200, 44.9100, 44.9400, 45.3700, 45.3600, 46.1400, 46.1600,
         46.0300, 46.6800, 44.1200, 42.2600, 43.1100, 43.7600, 44.3600, 44.6000,
         43.7500, 44.4700, 44.4200, 45.4800]]) shape : torch.Size([1, 300])

    d['decoder_cat']: tensor([], size=(1, 50, 0), dtype=torch.int64)

    d['decoder_cont']: tensor([[[-1.3044e+00, -1.4476e+00, -1.0544e+00, -9.0613e-01, -9.1462e-01,
          -8.8923e-01, -4.7397e-01, -5.4160e-01, -6.8538e-01, -1.2875e-01,
           3.4196e-01, -9.0092e-01, -8.9342e-01],
         [-1.3030e+00, -7.3274e-01, -1.0216e+00, -8.9466e-01, -9.0580e-01,
          -8.8757e-01, -3.6803e-01, -5.3600e-01, -6.5595e-01,  1.2588e+00,
           1.4903e+00, -8.9414e-01, -8.8988e-01],
         [-1.3015e+00, -1.7915e-02, -1.0251e+00, -8.8666e-01, -8.9899e-01,
          -8.8603e-01, -3.7409e-01, -5.3453e-01, -6.3413e-01, -1.7203e-01,
           9.8876e-01, -8.8905e-01, -8.8692e-01],
         [-1.3001e+00,  6.9691e-01, -1.0138e+00, -8.7683e-01, -8.9271e-01,
          -8.8411e-01, -5.4278e-01, -4.9533e-01, -6.0389e-01,  4.0471e-01,
           1.2394e+00, -8.8342e-01, -8.8346e-01],
         [-1.2987e+00,  1.4117e+00, -1.0176e+00, -8.7192e-01, -8.8793e-01,
          -8.8277e-01, -6.0707e-01, -4.7144e-01, -5.8203e-01, -1.8252e-01,
           5.4914e-01, -8.7930e-01, -8.8060e-01],
         [-1.2973e+00, -1.4476e+00, -1.0138e+00, -8.6615e-01, -8.8339e-01,
          -8.8129e-01, -9.0479e-01, -4.5444e-01, -5.5893e-01,  1.1048e-01,
           6.6677e-01, -8.7545e-01, -8.7775e-01],
         [-1.2958e+00, -7.3274e-01, -1.0086e+00, -8.6429e-01, -8.7672e-01,
          -8.8043e-01, -8.7393e-01, -5.4522e-01, -5.3991e-01,  1.6616e-01,
           1.3383e-01, -8.7162e-01, -8.7481e-01],
         [-1.2944e+00, -1.7915e-02, -1.0078e+00, -8.6181e-01, -8.7147e-01,
          -8.7955e-01, -9.2773e-01, -6.1223e-01, -5.2147e-01, -2.4012e-03,
           2.1584e-01, -8.6837e-01, -8.7210e-01],
         [-1.2930e+00,  6.9691e-01, -1.0176e+00, -8.6235e-01, -8.6682e-01,
          -8.7889e-01, -9.1064e-01, -7.7442e-01, -5.1069e-01, -4.1832e-01,
          -1.7963e-01, -8.6699e-01, -8.7032e-01],
         [-1.2916e+00,  1.4117e+00, -1.0202e+00, -8.6272e-01, -8.6455e-01,
          -8.7900e-01, -8.9170e-01, -8.4184e-01, -5.1278e-01, -1.3769e-01,
          -1.5825e-01, -8.6620e-01, -8.6888e-01],
         [-1.2901e+00, -1.4476e+00, -1.0329e+00, -8.6545e-01, -8.6302e-01,
          -8.7692e-01, -7.6402e-01, -9.5274e-01, -5.2022e-01, -5.3589e-01,
          -4.6478e-01, -8.6720e-01, -8.6844e-01],
         [-1.2887e+00, -7.3274e-01, -1.0095e+00, -8.6557e-01, -8.6215e-01,
          -8.7209e-01, -7.6883e-01, -9.4678e-01, -5.9413e-01,  8.8055e-01,
          -1.2577e-01, -8.6498e-01, -8.6645e-01],
         [-1.2873e+00, -1.7915e-02, -1.0005e+00, -8.6454e-01, -8.6040e-01,
          -8.6781e-01, -7.1355e-01, -9.2969e-01, -6.3830e-01,  3.1059e-01,
           2.4216e-02, -8.6200e-01, -8.6405e-01],
         [-1.2859e+00,  6.9691e-01, -9.9328e-01, -8.6107e-01, -8.5893e-01,
          -8.6395e-01, -6.1743e-01, -8.8580e-01, -6.6174e-01,  2.4185e-01,
           3.4351e-01, -8.5863e-01, -8.6138e-01],
         [-1.2845e+00,  1.4117e+00, -9.7591e-01, -8.5476e-01, -8.5596e-01,
          -8.6009e-01, -4.7640e-01, -7.9449e-01, -6.5395e-01,  6.2495e-01,
           7.1737e-01, -8.5362e-01, -8.5778e-01],
         [-1.2830e+00, -1.4476e+00, -9.8286e-01, -8.4762e-01, -8.5375e-01,
          -8.5673e-01, -6.7904e-01, -7.6113e-01, -6.5886e-01, -2.9941e-01,
           8.3440e-01, -8.5042e-01, -8.5500e-01],
         [-1.2816e+00, -7.3274e-01, -1.0028e+00, -8.4667e-01, -8.5333e-01,
          -8.5319e-01, -7.3027e-01, -7.6193e-01, -7.3555e-01, -8.0070e-01,
           1.3661e-02, -8.5039e-01, -8.5384e-01],
         [-1.2802e+00, -1.7915e-02, -1.0054e+00, -8.4737e-01, -8.5317e-01,
          -8.5050e-01, -6.9936e-01, -7.6263e-01, -7.9266e-01, -1.3659e-01,
          -2.0026e-01, -8.5070e-01, -8.5297e-01],
         [-1.2788e+00,  6.9691e-01, -1.0130e+00, -8.5018e-01, -8.5284e-01,
          -8.4802e-01, -6.1573e-01, -7.6939e-01, -8.7721e-01, -3.2762e-01,
          -4.7008e-01, -8.5193e-01, -8.5269e-01],
         [-1.2773e+00,  1.4117e+00, -9.9386e-01, -8.5274e-01, -8.5096e-01,
          -8.4595e-01, -7.2834e-01, -7.8581e-01, -9.0562e-01,  7.0107e-01,
          -4.3418e-01, -8.5046e-01, -8.5114e-01],
         [-1.2759e+00, -1.4476e+00, -1.0014e+00, -8.5538e-01, -8.4870e-01,
          -8.4406e-01, -8.5267e-01, -8.7424e-01, -9.5688e-01, -3.2516e-01,
          -4.4633e-01, -8.5024e-01, -8.5025e-01],
         [-1.2745e+00, -7.3274e-01, -9.9501e-01, -8.5426e-01, -8.4767e-01,
          -8.4311e-01, -8.2739e-01, -8.8869e-01, -9.5861e-01,  2.0874e-01,
           3.4407e-02, -8.4923e-01, -8.4901e-01],
         [-1.2731e+00, -1.7915e-02, -9.9328e-01, -8.5253e-01, -8.4716e-01,
          -8.4198e-01, -8.1579e-01, -8.8994e-01, -9.6512e-01,  3.0533e-02,
           1.1489e-01, -8.4818e-01, -8.4778e-01],
         [-1.2716e+00,  6.9691e-01, -9.7533e-01, -8.4717e-01, -8.4587e-01,
          -8.4061e-01, -7.7720e-01, -8.5474e-01, -9.4431e-01,  6.4684e-01,
           5.8929e-01, -8.4499e-01, -8.4544e-01],
         [-1.2702e+00,  1.4117e+00, -9.7504e-01, -8.4448e-01, -8.4581e-01,
          -8.3910e-01, -7.1477e-01, -8.5199e-01, -9.3149e-01, -2.5081e-02,
           2.3494e-01, -8.4234e-01, -8.4330e-01],
         [-1.2688e+00, -1.4476e+00, -9.6636e-01, -8.3949e-01, -8.4464e-01,
          -8.3741e-01, -7.0272e-01, -8.0440e-01, -9.0883e-01,  2.9117e-01,
           5.3539e-01, -8.3906e-01, -8.4078e-01],
         [-1.2674e+00, -7.3274e-01, -9.7099e-01, -8.3607e-01, -8.4236e-01,
          -8.3607e-01, -7.6454e-01, -7.8807e-01, -8.9527e-01, -2.1027e-01,
           3.3075e-01, -8.3697e-01, -8.3881e-01],
         [-1.2659e+00, -1.7915e-02, -9.6665e-01, -8.3227e-01, -8.3959e-01,
          -8.3461e-01, -9.2096e-01, -7.8289e-01, -8.8005e-01,  1.2736e-01,
           3.7789e-01, -8.3469e-01, -8.3673e-01],
         [-1.2645e+00,  6.9691e-01, -1.0161e+00, -8.3809e-01, -8.3982e-01,
          -8.3456e-01, -4.7625e-01, -7.7055e-01, -8.8144e-01, -1.9293e+00,
          -8.4734e-01, -8.3925e-01, -8.3822e-01],
         [-1.2631e+00,  1.4117e+00, -9.9009e-01, -8.4023e-01, -8.3955e-01,
          -8.3349e-01, -4.6669e-01, -7.7387e-01, -8.9656e-01,  9.6890e-01,
          -3.8167e-01, -8.3960e-01, -8.3779e-01],
         [-1.2617e+00, -1.4476e+00, -9.9125e-01, -8.4378e-01, -8.3883e-01,
          -8.3200e-01, -5.1292e-01, -7.8992e-01, -9.3765e-01, -8.0281e-02,
          -5.5672e-01, -8.4003e-01, -8.3749e-01],
         [-1.2603e+00, -7.3274e-01, -9.8430e-01, -8.4568e-01, -8.3807e-01,
          -8.3111e-01, -5.6364e-01, -7.9918e-01, -9.4666e-01,  2.2897e-01,
          -3.4959e-01, -8.3949e-01, -8.3674e-01],
         [-1.2588e+00, -1.7915e-02, -1.0167e+00, -8.5282e-01, -8.3974e-01,
          -8.3169e-01, -6.2396e-01, -7.3619e-01, -9.3059e-01, -1.2842e+00,
          -1.0091e+00, -8.4325e-01, -8.3826e-01],
         [-1.2574e+00,  6.9691e-01, -1.0005e+00, -8.5059e-01, -8.4154e-01,
          -8.3194e-01, -6.9922e-01, -7.3214e-01, -9.2827e-01,  5.9178e-01,
           1.8134e-01, -8.4422e-01, -8.3854e-01],
         [-1.2560e+00,  1.4117e+00, -9.9878e-01, -8.5183e-01, -8.4323e-01,
          -8.3276e-01, -7.1324e-01, -7.3794e-01, -9.3309e-01,  3.0800e-02,
          -2.6832e-01, -8.4479e-01, -8.3867e-01],
         [-1.2546e+00, -1.4476e+00, -9.9009e-01, -8.5166e-01, -8.4492e-01,
          -8.3301e-01, -7.0850e-01, -7.7687e-01, -9.3525e-01,  2.9683e-01,
          -8.8521e-02, -8.4413e-01, -8.3820e-01],
         [-1.2531e+00, -7.3274e-01, -1.0040e+00, -8.5447e-01, -8.4728e-01,
          -8.3305e-01, -7.7881e-01, -8.0763e-01, -9.3458e-01, -5.6959e-01,
          -4.6774e-01, -8.4540e-01, -8.3872e-01],
         [-1.2517e+00, -1.7915e-02, -1.0086e+00, -8.5331e-01, -8.5027e-01,
          -8.3317e-01, -8.5348e-01, -8.7970e-01, -9.3216e-01, -2.1509e-01,
           4.1280e-02, -8.4703e-01, -8.3950e-01],
         [-1.2503e+00,  6.9691e-01, -1.0083e+00, -8.5443e-01, -8.4972e-01,
          -8.3300e-01, -8.3042e-01, -8.9960e-01, -9.3658e-01, -2.4813e-02,
          -2.5354e-01, -8.4833e-01, -8.4019e-01],
         [-1.2489e+00,  1.4117e+00, -1.0080e+00, -8.5575e-01, -8.5100e-01,
          -8.3351e-01, -8.2631e-01, -9.0383e-01, -9.3045e-01, -2.4815e-02,
          -2.7996e-01, -8.4935e-01, -8.4080e-01],
         [-1.2474e+00, -7.3274e-01, -1.0095e+00, -8.5851e-01, -8.5230e-01,
          -8.3380e-01, -9.7964e-01, -9.1029e-01, -9.2540e-01, -9.2081e-02,
          -4.6396e-01, -8.5038e-01, -8.4144e-01],
         [-1.2460e+00, -1.7915e-02, -1.0049e+00, -8.5864e-01, -8.5377e-01,
          -8.3415e-01, -9.8957e-01, -9.5963e-01, -9.2231e-01,  1.4316e-01,
          -1.2571e-01, -8.5062e-01, -8.4171e-01],
         [-1.2446e+00,  6.9691e-01, -1.0075e+00, -8.5847e-01, -8.5310e-01,
          -8.3465e-01, -9.9085e-01, -9.8278e-01, -9.1804e-01, -1.3674e-01,
          -8.8251e-02, -8.5115e-01, -8.4213e-01],
         [-1.2432e+00,  1.4117e+00, -1.0694e+00, -8.6718e-01, -8.5802e-01,
          -8.3800e-01, -2.9772e-01, -6.8117e-01, -8.1964e-01, -2.4893e+00,
          -1.2403e+00, -8.5962e-01, -8.4672e-01],
         [-1.2418e+00, -1.4476e+00, -1.0442e+00, -8.7234e-01, -8.6126e-01,
          -8.4046e-01, -2.7496e-01, -6.4414e-01, -7.9923e-01,  9.7517e-01,
          -7.7935e-01, -8.6328e-01, -8.4916e-01],
         [-1.2403e+00, -7.3274e-01, -1.0688e+00, -8.8080e-01, -8.6688e-01,
          -8.4410e-01, -1.9448e-01, -5.7487e-01, -7.5933e-01, -1.0237e+00,
          -1.2091e+00, -8.6946e-01, -8.5304e-01],
         [-1.2389e+00, -1.7915e-02, -1.0700e+00, -8.9008e-01, -8.7159e-01,
          -8.4762e-01, -3.1313e-01, -5.2173e-01, -7.3010e-01, -8.2963e-02,
          -1.3123e+00, -8.7467e-01, -8.5663e-01],
         [-1.2375e+00,  6.9691e-01, -1.0532e+00, -8.9660e-01, -8.7477e-01,
          -8.5070e-01, -7.2303e-01, -5.2586e-01, -7.4287e-01,  6.4056e-01,
          -9.5579e-01, -8.7676e-01, -8.5874e-01],
         [-1.2361e+00,  1.4117e+00, -1.0720e+00, -8.9697e-01, -8.7932e-01,
          -8.5269e-01, -7.0960e-01, -5.1361e-01, -7.0318e-01, -7.9485e-01,
          -1.6018e-01, -8.8090e-01, -8.6192e-01],
         [-1.2346e+00, -1.4476e+00, -1.0674e+00, -9.0027e-01, -8.8355e-01,
          -8.5544e-01, -8.3675e-01, -5.4027e-01, -6.8935e-01,  1.5178e-01,
          -5.4998e-01, -8.8370e-01, -8.6449e-01]]]) shape: torch.Size([1, 50, 13])

    d['decoder_lengths']: tensor([50])

    d['decoder_target']: tensor([[45.4000, 46.5300, 46.4100, 46.8000, 46.6700, 46.8000, 46.9800, 47.0100,
         46.6700, 46.5800, 46.1400, 46.9500, 47.2600, 47.5100, 48.1100, 47.8700,
         47.1800, 47.0900, 46.8300, 47.4900, 47.2300, 47.4500, 47.5100, 48.1300,
         48.1400, 48.4400, 48.2800, 48.4300, 46.7200, 47.6200, 47.5800, 47.8200,
         46.7000, 47.2600, 47.3200, 47.6200, 47.1400, 46.9800, 46.9900, 47.0000,
         46.9500, 47.1100, 47.0200, 44.8800, 45.7500, 44.9000, 44.8600, 45.4400,
         44.7900, 44.9500]]) shape: torch.Size([1, 50])
    
    d['target_scale']: tensor([[81.8250, 34.5472]])




    '''
    output[0] = tensor of shape [2, 50, 7] # 7 because we have 7 quantiles, so 2 batches, 50 forward proj, 7 quantiles, quantile 0.5 is the prediction so it would be index 3 for the prediction
    output[1] = tensor of shape [2, 300, 1] # 2 batches, 300 for the lookback
    output[2] = tuple of length 4: 
        output[2][0]: tensor of shape [2, 300, 1]
        output[2][1]: tensor of shape [2, 300, 1]
        output[2][2]: tensor of shape [2, 300, 1]
        output[2][3]: tensor of shape [2, 300, 1]
    output[3] = tuple of length 4:
        output[3][0]: tensor of shape [2, 50, 7] # these ones would be the scaling, to scale back to raw adjprc rather than the robust scaled adjprc
        output[3][1]: tensor of shape [2, 50, 7]
        output[3][2]: tensor of shape [2, 50, 7]
        output[3][3]: tensor of shape [2, 50, 7]

    '''